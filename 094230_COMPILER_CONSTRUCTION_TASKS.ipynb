{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNvSB+26W1zcXBri6WLLBKW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njonge-nathan/compiler_construction_task/blob/main/094230_COMPILER_CONSTRUCTION_TASKS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1"
      ],
      "metadata": {
        "id": "J7_RrkCLjtPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Write a program to identify programming lines (in the same programming language) as other comments or not.\n",
        "\n",
        "Logic:\n",
        "\n",
        "Split the program lines into tokens.\n",
        "\n",
        "1. Split the program lines into tokens.\n",
        "2. For each token, check if it is a comment delimiter.\n",
        "3. If the token is a comment delimiter, then the rest of the line is a comment.\n",
        "4. Otherwise, the line is not a comment.\n",
        "\n",
        "Lexical analysis concepts:\n",
        "\n",
        "Lexical analysis is the process of breaking down a program into tokens. Tokens are the basic building blocks of a program, such as keywords, identifiers, operators, and symbols.\n",
        "\n",
        "The logic for Task 1 uses lexical analysis to split the program lines into tokens. This is necessary because comments can start anywhere on a line, so it is important to identify the tokens in order to determine whether or not the line is a comment.\n",
        "\n"
      ],
      "metadata": {
        "id": "PyQok4-7wnVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_comment(line):\n",
        "  \"\"\"Returns True if the line is a comment, False otherwise.\"\"\"\n",
        "\n",
        "  comment_delimiter = \"#\"  # Comment delimiter for Python\n",
        "\n",
        "  # Split the line into tokens.\n",
        "  tokens = line.split()\n",
        "\n",
        "  # If the first token is a comment delimiter, then the rest of the line is a comment.\n",
        "  if tokens[0] == comment_delimiter:\n",
        "    return True\n",
        "\n",
        "  # Otherwise, the line is not a comment.\n",
        "  return False\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "line = \"This is a comment. # This is also a comment.\"\n",
        "\n",
        "if is_comment(line):\n",
        "  print(\"The line is a comment.\")\n",
        "else:\n",
        "  print(\"The line is not a comment.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41n2G4-1jzqN",
        "outputId": "09148b38-e449-4abd-e71b-c9c41fe1e6c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The line is not a comment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "kbPeBgn-xUUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Write a program to test whether a given identifier is valid or not.\n",
        "\n",
        "Logic:\n",
        "\n",
        "1. Check if the identifier starts with a letter or an underscore.\n",
        "2. Check if the identifier contains only letters, numbers, and underscores.\n",
        "3. Check if the identifier is not a reserved keyword."
      ],
      "metadata": {
        "id": "MtmkOQuwxXht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_identifier(identifier):\n",
        "  \"\"\"Returns True if the identifier is valid, False otherwise.\"\"\"\n",
        "\n",
        "  # Check if the identifier starts with a letter or an underscore.\n",
        "  if not identifier[0].isalpha() and identifier[0] != \"_\":\n",
        "    return False\n",
        "\n",
        "  # Check if the identifier contains only letters, numbers, and underscores.\n",
        "  for char in identifier:\n",
        "    if not char.isalnum() and char != \"_\":\n",
        "      return False\n",
        "\n",
        "  # Check if the identifier is not a reserved keyword.\n",
        "  reserved_keywords = [\"and\", \"as\", \"assert\", \"break\", \"class\", \"continue\", \"def\", \"del\", \"elif\", \"else\", \"except\", \"exec\", \"finally\", \"for\", \"from\", \"global\", \"if\", \"import\", \"in\", \"is\", \"lambda\", \"not\", \"or\", \"pass\", \"print\", \"raise\", \"return\", \"try\", \"while\", \"with\", \"yield\"]\n",
        "\n",
        "  if identifier in reserved_keywords:\n",
        "    return False\n",
        "\n",
        "  # Otherwise, the identifier is valid.\n",
        "  return True\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "identifier = \"my_identifier\"\n",
        "\n",
        "if is_valid_identifier(identifier):\n",
        "  print(\"The identifier is valid.\")\n",
        "else:\n",
        "  print(\"The identifier is not valid.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGhOtThWxRbQ",
        "outputId": "b536a9bb-12f5-4a4f-bfc8-f348b4ac976b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The identifier is valid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3:\n",
        "\n",
        "Write a program to construct the LL(1) table for a given CFG.\n",
        "\n",
        "Logic:\n",
        "\n",
        "1. For each non-terminal symbol in the CFG, create a table entry for each production rule that starts with that symbol.\n",
        "2. For each table entry, determine the first and follow sets of the non-terminal symbol.\n",
        "3. For each table entry, determine the action to take based on the first and follow sets.\n",
        "\n",
        "Syntax analysis concepts:\n",
        "\n",
        "Syntax analysis is necessary for Task 3, because the logic needs to determine the first and follow sets of the non-terminal symbols in the CFG. The first and follow sets are used to determine the action to take for each table entry."
      ],
      "metadata": {
        "id": "-MMV1aI4yxGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LL(1) parsing\n",
        "\n",
        "LL(1) parsing is a technique for parsing expressions that takes into account the precedence of operators. In LL(1) parsing, the parser uses the first symbol of the input stream to decide which production rule to apply.\n",
        "\n",
        "LL(1) table\n",
        "\n",
        "An LL(1) table is a table that maps the first symbol of the input stream to the production rule that should be applied.\n",
        "\n",
        "Solution\n",
        "\n",
        "To construct the LL(1) table for a given CFG, we can use the following algorithm:\n",
        "\n",
        "1. For each production rule in the CFG, compute the first and follow sets of the non-terminal symbol on the left-hand side of the rule.\n",
        "2. For each non-terminal symbol in the CFG, compute the LL(1) table entry for each terminal symbol in the first set of the non-terminal symbol.\n",
        "\n",
        "The first and follow sets are used to determine which production rule to apply when the parser encounters a given terminal symbol in the input stream."
      ],
      "metadata": {
        "id": "qsUm54Z00OC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CFG:\n",
        "\n",
        "S -> aAb | b\n",
        "\n",
        "First sets:\n",
        "\n",
        "First(S) = {a, b}\n",
        "First(A) = {a}\n",
        "\n",
        "Follow sets:\n",
        "\n",
        "Follow(S) = {}\n",
        "Follow(A) = {b}\n",
        "\n",
        "LL(1) table:\n",
        "\n",
        "Terminal symbol | Non-terminal symbol\n",
        "------- | --------\n",
        "a | S -> aAb\n",
        "a | A -> a\n",
        "b | S -> b"
      ],
      "metadata": {
        "id": "el3ez2-Dy_ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4(a):\n",
        "Bonus tasks (Optional to do):\n",
        "\n",
        "(a) Using any two files of YACC and Flex, show how a parser is generated.\n",
        "\n",
        "Logic:\n",
        "\n",
        "1. Create a YACC grammar file that defines the syntax of the language that you want to parse.\n",
        "2. Create a Flex lexer file that identifies the tokens in the language.\n",
        "3. Run the YACC grammar file and the Flex lexer file through the YACC parser generator to generate a parser.\n",
        "\n",
        "Lexical analysis concepts:\n",
        "\n",
        "The Flex lexer file is used to identify the tokens in the language. This is necessary because the YACC parser generator needs to know the tokens in the language in order to generate a parser.\n",
        "\n",
        "Syntax analysis concepts:\n",
        "\n",
        "The YACC grammar file is used to define the syntax of the language. The YACC parser generator uses the grammar file to generate a parser that can parse the language."
      ],
      "metadata": {
        "id": "eFmF4JYOzaZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yacc File (.y)"
      ],
      "metadata": {
        "id": "5cPapRRf2x7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%{\n",
        "#include <ctype.h>\n",
        "#include <stdio.h>\n",
        "#define YYSTYPE double /* double type for yacc stack */\n",
        "%}\n",
        "\n",
        "%%\n",
        "Lines : Lines S '\\n' { printf(\"OK \\n\"); }\n",
        "\t| S '\\n’\n",
        "\t| error '\\n' {yyerror(\"Error: reenter last line:\");\n",
        "\t\t\t\t\t\tyyerrok; };\n",
        "S\t : '(' S ')’\n",
        "\t| '[' S ']’\n",
        "\t| /* empty */ ;\n",
        "%%\n",
        "\n",
        "#include \"lex.yy.c\"\n",
        "\n",
        "void yyerror(char * s)\n",
        "/* yacc error handler */\n",
        "{\n",
        "fprintf (stderr, \"%s\\n\", s);\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "return yyparse();\n",
        "}\n"
      ],
      "metadata": {
        "id": "dcEFrtdpzocu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lex File (.l)"
      ],
      "metadata": {
        "id": "NaRRWa672wdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%{\n",
        "%}\n",
        "\n",
        "%%\n",
        "[ \\t]\t { /* skip blanks and tabs */ }\n",
        "\\n|.\t { return yytext[0]; }\n",
        "%%\n"
      ],
      "metadata": {
        "id": "OmaU2YN62PZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write lex program in a file file.l and yacc in a file file.y\n",
        "\n",
        "Open Terminal and Navigate to the Directory where you have saved the files.\n",
        "\n",
        "type lex file.l\n",
        "\n",
        "type yacc file.y\n",
        "\n",
        "type cc lex.yy.c y.tab.h -ll\n",
        "\n",
        "type ./a.out"
      ],
      "metadata": {
        "id": "y_o9mX4X3pX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4(b):"
      ],
      "metadata": {
        "id": "v5mteVbCyHsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Write a program to implement operator precedence parsing.\n",
        "\n",
        "Logic:\n",
        "\n",
        "1. Create a stack to store the operators that have been encountered.\n",
        "2. When an operator is encountered, compare its precedence to the precedence of the operator at the top of the stack.\n",
        "3. If the precedence of the new operator is higher than the precedence of the operator at the top of the stack, then pop the operator at the top of the stack and evaluate it.\n",
        "4. Push the new operator onto the stack.\n",
        "5. Continue processing the operators in the stack until the stack is empty.\n",
        "\n",
        "Lexical analysis concepts:\n",
        "\n",
        "Lexical analysis is necessary for Task 4(b), because the logic needs to identify the operators in the expression in order to determine their precedence."
      ],
      "metadata": {
        "id": "RkH-_qfuyPer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def operator_precedence(operator):\n",
        "  \"\"\"Returns the precedence of the operator.\"\"\"\n",
        "\n",
        "  operator_precedence_table = {\n",
        "      \"+\": 1,\n",
        "      \"-\": 1,\n",
        "      \"*\": 2,\n",
        "      \"/\": 2,\n",
        "      \"^\": 3,\n",
        "  }\n",
        "\n",
        "  return operator_precedence_table[operator]\n",
        "\n",
        "\n",
        "def operator_precedence_parsing(expression):\n",
        "  \"\"\"Parses the expression using operator precedence parsing.\"\"\"\n",
        "\n",
        "  operator_stack = []\n",
        "  output_queue = []\n",
        "\n",
        "  for token in expression:\n",
        "    if token in [\"+\", \"-\", \"*\", \"/\", \"^\"]:\n",
        "      # If the operator stack is not empty and the precedence of the new operator is\n",
        "      # lower than or equal to the precedence of the operator at the top of the stack,\n",
        "      # then pop the operator at the top of the stack and evaluate it.\n",
        "      while operator_stack and operator_precedence(token) <= operator_precedence(operator_stack[-1]):\n",
        "        output_queue.append(operator_stack.pop())\n",
        "\n",
        "      # Push the new operator onto the stack.\n",
        "      operator_stack.append(token)\n",
        "    else:\n",
        "      # If the token is an operand, then add it to the output queue.\n",
        "      output_queue.append(token)\n",
        "\n",
        "  # Pop the remaining operators from the stack and evaluate them.\n",
        "  while operator_stack:\n",
        "    output_queue.append(operator_stack.pop())\n",
        "\n",
        "  return output_queue\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "expression = \"2 + 3 * 4 + 5 ^ 2\"\n",
        "\n",
        "output_queue = operator_precedence_parsing(expression)\n",
        "\n",
        "print(output_queue)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0uTg5odx-I1",
        "outputId": "c8047135-0805-440d-99da-d6bb9bf803a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2', ' ', ' ', '3', ' ', ' ', '4', ' ', '*', '+', ' ', '5', ' ', ' ', '2', '^', '+']\n"
          ]
        }
      ]
    }
  ]
}